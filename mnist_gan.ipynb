{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_gan.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NamrathaKaranth/30-Days-of-Deep-Learning/blob/main/mnist_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMjYyUdo04ve",
        "outputId": "5ffb87ba-4857-4e1b-945c-3bcdcc5d2840",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPwCGfWA7E0N",
        "outputId": "d1a88755-5739-4f2a-dfd2-71a59b4fe4e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd  \"/content/gdrixve/Shared drives/Datasets/sridhar-30-days\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/gdrixve/Shared drives/Datasets/sridhar-30-days'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZdCkUOP7buO"
      },
      "source": [
        "!mkdir gan_mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAovkJqR7gfq",
        "outputId": "9f10677c-4635-44b2-ea74-93af3e3ed5d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd  \"/content/gdrive/Shared drives/Datasets/sridhar-30-days/gan_mnist\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/Shared drives/Datasets/sridhar-30-days/gan_mnist\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHiQL-7u7iMd"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as Optim\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep1Lssq87rU7"
      },
      "source": [
        "transform_option = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n",
        "image_train = MNIST(root=\"./data/MNIST\", download=True, train=True, transform=transform_option)\n",
        "train_data_loader = DataLoader(image_train, batch_size=32, shuffle=True)\n",
        "image_test = MNIST(root=\"./data/MNIST\", download=True, train=False, transform=transform_option)\n",
        "test_data_loader = DataLoader(image_test, batch_size=32, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yvgWbUN_fQj"
      },
      "source": [
        "class MNISTDiscriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MNISTDiscriminator, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 24, 3)\n",
        "    self.conv2 = nn.Conv2d(24, 48, 3)\n",
        "    self.fc1 = nn.Linear(48, 120)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "    x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "    x = torch.sigmoid(self.fc1(x))\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oerMaJoX-Wm4"
      },
      "source": [
        "class MNISTGenerator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MNISTGenerator, self).__init__()\n",
        "    self.conv1 = nn.ConvTranspose2d(100, 24, 3)\n",
        "    self.conv2 = nn.ConvTranspose2d(24, 1, 3)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = F.relu(nn.BatchNorm2d(self.conv1(x)))\n",
        "    x = F.relu(self.conv2(x))\n",
        "    \n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h5h1lt-Ck34"
      },
      "source": [
        "discriminator = MNISTDiscriminator()\n",
        "generator = MNISTGenerator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukz2A8H0DBV7"
      },
      "source": [
        "gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1soF87MDe3M"
      },
      "source": [
        "generator = generator.to(gpu)\n",
        "discriminator = discriminator.to(gpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toX1-dStEdGu",
        "outputId": "32b34697-8971-4e2b-f6e3-3cba7c820b76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "generator.parameters()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7f34f5c59200>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kKCug4fD-RN"
      },
      "source": [
        "generator_optimizer = Optim.Adam(params=generator.parameters(), lr=0.01)\n",
        "discriminator_optimizer = Optim.Adam(params=discriminator.parameters(), lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUC4JKzrExpv"
      },
      "source": [
        "generator.train()\n",
        "discriminator.train()\n",
        "generator_loss_for_epoch = []\n",
        "discriminator_loss_for_epoch = []\n",
        "number_of_batch = 0\n",
        "\n",
        "for epoch in range(100):\n",
        "  for image_batch, item in train_data_loader:\n",
        "    print( image_batch.size(0))\n",
        "    image_batch = image_batch.to(gpu)\n",
        "    label_real = torch.ones(image_batch.size(0), device=gpu)\n",
        "    label_fake =  torch.zeros(image_batch.size(0), device=gpu)\n",
        "    noise = torch.randn(image_batch.size(0), 100,1,1, device=gpu )\n",
        "    generator_image_batch = generator(noise)\n",
        "\n",
        "\n",
        "    real_prediction = discriminator(image_batch).squeeze()\n",
        "    fake_prediction = discriminator(generator_image_batch.detach()).squeeze()\n",
        "    discriminator_loss = 0.5 * (F.binary_cross_entropy(real_prediction,label_real) + F.binary_cross_entropy(fake_prediction,label_fake))\n",
        "    discriminator_optimizer.zero_grad()\n",
        "    discriminator_loss.backward()\n",
        "    discriminator_optimizer.step()\n",
        "\n",
        "\n",
        "    fake_prediction = discriminator(generator_image_batch).squeeze()\n",
        "    generator_loss = F.binary_cross_entropy(fake_prediction,label_real)\n",
        "    generator_optimizer.zero_grad()\n",
        "    generator_loss.backward()\n",
        "    generator_optimizer.step()\n",
        "    \n",
        "    generator_loss_for_epoch[-1] += generator_loss.item()\n",
        "    discriminator_loss_for_epoch[-1] += discriminator_loss.item()\n",
        "\n",
        "    number_of_batch += 1\n",
        "  print(\"Previously performed epoch training:\" + epoch + \"number\")\n",
        "  print(\"Previously performed epoch generator loss:\" + generator_loss_for_epoch[-1] / number_of_batch)\n",
        "  print(\"Previously performed epoch discriminator loss:\" + discriminator_loss_for_epoch[-1] / number_of_batch)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}